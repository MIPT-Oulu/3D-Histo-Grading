{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import xlsxwriter\n",
    "import pandas as pd # Excel\n",
    "import struct # Binary writing\n",
    "\n",
    "import scipy.io as sio # Read .mat files\n",
    "import h5py\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "import scipy.signal\n",
    "import scipy.ndimage\n",
    "\n",
    "import sklearn.metrics as skmet\n",
    "import sklearn.decomposition as skdec\n",
    "import sklearn.linear_model as sklin\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test image\n",
    "def test_image():\n",
    "    test_image = np.zeros((20, 30))\n",
    "    for i in range(20):\n",
    "        for j in range(30):\n",
    "            test_image[i,j] = i + j\n",
    "    return test_image\n",
    "\n",
    "#Regression\n",
    "def regress(features,score):\n",
    "    pred = []\n",
    "    #Leave one out split\n",
    "    loo = LeaveOneOut()\t\n",
    "    for trainidx, testidx in loo.split(features):\n",
    "        #Indices\n",
    "        X_train, X_test = features[trainidx], features[testidx]\n",
    "        X_test -= X_train.mean(0)\n",
    "        X_train -= X_train.mean(0)\n",
    "\n",
    "        Y_train, Y_test = score[trainidx], score[testidx]\t\t\n",
    "        #Linear regression\t\t\n",
    "        regr = sklin.Ridge(alpha=1)\n",
    "        regr.fit(X_train,Y_train)\n",
    "        #Predicted score\t\t\n",
    "        pred.append(regr.predict(X_test))\n",
    "\n",
    "    return np.array(pred)\n",
    "\n",
    "#Logistic regression\n",
    "def logreg(features,score):\n",
    "    pred = []\n",
    "    #Leave one out split\n",
    "    loo = LeaveOneOut()\t\n",
    "    for trainidx, testidx in loo.split(features):\n",
    "        #Indices\n",
    "        X_train, X_test = features[trainidx], features[testidx]\n",
    "        X_test -= X_train.mean(0)\n",
    "        X_train -= X_train.mean(0)\n",
    "\n",
    "        Y_train, Y_test = score[trainidx], score[testidx]\t\t\n",
    "        #Linear regression\n",
    "        regr = sklin.LogisticRegression(solver='newton-cg',max_iter=1000)\n",
    "        regr.fit(X_train,Y_train)\n",
    "        #Predicted score\n",
    "        P = regr.predict_proba(X_test)\n",
    "        pred.append(P)\n",
    "\n",
    "    pred = np.array(pred)\n",
    "    pred = pred[:,:,1]\n",
    "    return pred.flatten()\n",
    "\n",
    "#MRELBP\n",
    "def MRELBP(im,N,R,r,w_c,w_r):\n",
    "    #Takes Median Robust Extended Local Binary Pattern from image im\n",
    "    #Uses N neighbours from radii R and r, R must be larger than r\n",
    "    #Median filter uses kernel sizes w_c for center pixels, w_r[0] for larger radius and w_r[1]\n",
    "    #for smaller radius\t\n",
    "    #Grayscale values are centered at their mean and scales with global standad deviation\n",
    "\n",
    "    #Mean grayscale value and std\n",
    "    muI = im.mean()\n",
    "    stdI = im.std()\n",
    "\n",
    "    #Centering and scaling with std\n",
    "    I = (im-muI)/stdI\n",
    "\n",
    "    #Median filtering\n",
    "    Ic = scipy.signal.medfilt(I,w_c)\n",
    "    #Center pixels\n",
    "    d = round(R+(w_r[0]-1)/2)\n",
    "    Ic = Ic[d:-d,d:-d]\n",
    "    #Subtracting the mean pixel value from center pixels\n",
    "    Ic = Ic-Ic.mean()\n",
    "    #Bining center pixels\n",
    "    Chist = np.zeros((1,2))\n",
    "    Chist[0,0] = np.sum(Ic>=-1e-06)\n",
    "    Chist[0,1] = np.sum(Ic<-1e-06)\n",
    "\n",
    "    #Median filtered images for large and small radius\n",
    "    IL = scipy.signal.medfilt(I,w_r[0])\n",
    "    d1 = round((w_r[0]-1)/2)\n",
    "    IL = IL[d1:-d1,d1:-d1]\n",
    "    IS = scipy.signal.medfilt2d(I,w_r[1])\n",
    "    d2 = round((w_r[1]-1)/2)\n",
    "    IS = IS[d2:-d2,d2:-d2]\n",
    "\n",
    "    #print('Center filtered') # Filtered image    \n",
    "    #print(Ic[0,0:30]) # Filtered image\n",
    "\n",
    "    #row1,col1 = np.shape(IS)    \n",
    "    #print(\"Size (IS): \" + str(row1) + \", \" + str(col1))\n",
    "    # Save filtered images\n",
    "    #Iconvert1 = (Ic-np.min(Ic))/(np.max(Ic)-np.min(Ic))*255 \n",
    "    #cv2.imwrite(r'C:\\Users\\sarytky\\Desktop\\trials\\Median_python_c.png', Iconvert1) # not scaled to 0-255\n",
    "    #Iconvert2 = (IS-np.min(IS))/(np.max(IS)-np.min(IS))*255 \n",
    "    #cv2.imwrite(r'C:\\Users\\sarytky\\Desktop\\trials\\Median_python_s.png', Iconvert2) # not scaled to 0-255\n",
    "    #Iconvert3 = (IL-np.min(IL))/(np.max(IL)-np.min(IL))*255 \n",
    "    #cv2.imwrite(r'C:\\Users\\sarytky\\Desktop\\trials\\Median_python_l.png', Iconvert3) # not scaled to 0-255\n",
    "\n",
    "    #Neighbours\n",
    "    pi = np.pi\n",
    "    #Empty arrays for the neighbours\n",
    "    row,col = np.shape(Ic)\n",
    "    NL = np.zeros((row,col,N))\n",
    "    NS = np.zeros((row,col,N))\n",
    "    #print(\"Size (Ic): \" + str(row) + \", \" + str(col))\n",
    "    for k in range(N):\n",
    "        #Angle to the neighbour\n",
    "        theta = 0+k*(-1*2*pi/N)\n",
    "        #Large neighbourhood\n",
    "        x = R+R*np.cos(theta)\n",
    "        y = R+R*np.sin(theta)\n",
    "        if abs(x-round(x)) < 1e-06 and abs(y-round(y)) < 1e-06:\n",
    "            x = int(round(x))\n",
    "            y = int(round(y))\n",
    "            P = IL[y:y+row,x:x+col]\n",
    "        else:\n",
    "            P = imbilinear(IL,col,x,row,y)\n",
    "        NL[:,:,k] = P\n",
    "        #Small neighbourhood\n",
    "        x = R+r*np.cos(theta)\n",
    "        y = R+r*np.sin(theta)\n",
    "        if abs(x-round(x)) < 1e-06 and abs(y-round(y)) < 1e-06:\n",
    "            x = int(round(x))\n",
    "            y = int(round(y))\n",
    "            P = IS[y:y+row,x:x+col]\n",
    "        else:\n",
    "            P = imbilinear(IS,col,x,row,y)\n",
    "        NS[:,:,k] = P\n",
    "    #Thresholding\n",
    "\n",
    "    #Thresholding radial neighbourhood\n",
    "    NR = NL-NS\n",
    "\n",
    "    #Subtraction of means\n",
    "    #Large neighbourhood\n",
    "    NLmu = NL.mean(axis=2)\t\t\n",
    "    #Small neighbouhood\n",
    "    NSmu = NS.mean(axis=2)\n",
    "\n",
    "    for k in range(N):\n",
    "        NL[:,:,k] = NL[:,:,k]-NLmu\n",
    "        NS[:,:,k] = NS[:,:,k]-NSmu\t\n",
    "\n",
    "    #Converting to binary images and taking the lbp values\n",
    "\n",
    "    #Initialization of arrays\n",
    "    lbpIL = np.zeros((row,col))\n",
    "    lbpIS = np.zeros((row,col))\n",
    "    lbpIR = np.zeros((row,col))\n",
    "\n",
    "    for k in range(N):\n",
    "        lbpIL = lbpIL+(NL[:,:,k]>=-1e-06)*2**k # NOTE ACCURACY FOR THRESHOLDING!!!\n",
    "        lbpIS = lbpIS+(NS[:,:,k]>=-1e-06)*2**k\n",
    "        lbpIR = lbpIR+(NR[:,:,k]>=-1e-06)*2**k\n",
    "\n",
    "    #Binning\n",
    "    Lhist = np.zeros((1,2**N))\n",
    "    Shist = np.zeros((1,2**N))\n",
    "    Rhist = np.zeros((1,2**N))\n",
    "    for k in range(2**N):\n",
    "        Lhist[0,k] = np.sum(lbpIL==k)\n",
    "        Shist[0,k] = np.sum(lbpIS==k)\n",
    "        Rhist[0,k] = np.sum(lbpIR==k)\n",
    "\n",
    "    #Chist = 1/np.linalg.norm(Chist)*Chist\n",
    "    #Lhist = 1/np.linalg.norm(Lhist)*Lhist\n",
    "    #Shist = 1/np.linalg.norm(Shist)*Shist\n",
    "    #Rhist = 1/np.linalg.norm(Rhist)*Rhist\n",
    "    return Chist,Lhist,Shist,Rhist, lbpIL, lbpIS, lbpIR\n",
    "\n",
    "#Mapping\n",
    "def getmapping(N):\n",
    "    #Defines rotation invariant uniform mapping for lbp of N neighbours\n",
    "    newMax = N + 2\n",
    "    table = np.zeros((1,2**N))\n",
    "    for k in range(2**N):\n",
    "        #Binary representation of bin number\n",
    "        binrep = np.binary_repr(k,N)\n",
    "        #Convert string to list of digits\n",
    "        i_bin = np.zeros((1,len(binrep)))\n",
    "        for ii in range(len(binrep)):\n",
    "            i_bin[0,ii] = int(float(binrep[ii]))\n",
    "        #Rotation\n",
    "        j_bin = np.roll(i_bin,-1)\n",
    "        #uniformity\n",
    "        numt = np.sum(i_bin!=j_bin)\t\t\n",
    "        #Binning\n",
    "        if numt <= 2:\n",
    "            b = np.binary_repr(k,N)\n",
    "            c=0\n",
    "            for ii in range(len(b)):\n",
    "                c = c+int(float(b[ii]))\n",
    "            table[0,k] = c\n",
    "        else:\n",
    "            table[0,k] = N+1\n",
    "    #num = newMax\n",
    "    return table\n",
    "\n",
    "#Apply mapping to lbp\n",
    "def maplbp(bin,mapping):\n",
    "    #Applies mapping to lbp bin\n",
    "    #Number of bins in output\n",
    "    N = int(np.max(mapping))\n",
    "    #print(N)\n",
    "    #Empty array\n",
    "    outbin = np.zeros((1,N+1))\n",
    "    for k in range(N+1):\n",
    "        #RIU indices\n",
    "        M = mapping==k\n",
    "        #Extract indices from original bin to new bin\n",
    "        outbin[0,k] = np.sum(M*bin)\n",
    "    return outbin\n",
    "\n",
    "#Bilinear interpolation\n",
    "def imbilinear(im,col,x,row,y):\n",
    "    #Takes bilinear interpotalion from image\n",
    "    #Starts from coordinates [y,x], ends at row,col\n",
    "    x1 = int(np.floor(x))\n",
    "    x2 = int(np.ceil(x))\n",
    "    y1 = int(np.floor(y))\n",
    "    y2 = int(np.ceil(y))\n",
    "    Q11 = im[y1:y1+row,x1:x1+col]\n",
    "    Q21 = im[y1:y1+row,x2:x2+col]\n",
    "    Q12 = im[y2:y2+row,x1:x1+col]\n",
    "    Q22 = im[y2:y2+row,x2:x2+col]\n",
    "    R1 = ((x2-x)/(x2-x1+1e-12))*Q11+((x-x1)/(x2-x1+1e-12))*Q21\n",
    "    R2 = ((x2-x)/(x2-x1+1e-12))*Q12+((x-x1)/(x2-x1+1e-12))*Q22\n",
    "    P = ((y2-y)/(y2-y1+1e-12))*R1+((y-y1)/(y2-y1+1e-12))*R2\n",
    "    return P\n",
    "\n",
    "# Image padding\n",
    "def impadding(im, padlength):\n",
    "    row,col = np.shape(im)\n",
    "    im_pad = np.zeros((row + 2 * padlength, col + 2 * padlength))\n",
    "    # Center\n",
    "    im_pad[padlength:-padlength, padlength:-padlength] = im\n",
    "    plt.imshow(im_pad)\n",
    "    plt.show()\n",
    "\n",
    "    return im_pad\n",
    "\n",
    "#Scikit PCA\n",
    "def ScikitPCA(features,ncomp):\n",
    "    pca = skdec.PCA(n_components=ncomp, svd_solver='full')\n",
    "    score = pca.fit(features).transform(features)\n",
    "    return pca, score\n",
    "\n",
    "#Principal component analysis\n",
    "def PCA(features,ncomp):\t\n",
    "    #Feature dimension, x=num variables,N=num observations\n",
    "    x,N = np.shape(features)\n",
    "    #Mean feature\n",
    "    mean_f = np.mean(features,axis=1)\n",
    "    #Centering\n",
    "    centrd = np.zeros((x,N))\n",
    "    for k in range(N):\n",
    "        centrd[:,k] = features[:,k]-mean_f\n",
    "\n",
    "    #PCs from covariance matrix if N>=x, svd otherwise\n",
    "    if False:\n",
    "        #Covariance matrix\n",
    "        Cov = np.zeros((x,x))\n",
    "        f = np.zeros((x,1))\n",
    "        for k in range(N):\t\t\n",
    "            f[:,0] = centrd[:,k]\n",
    "            Cov = Cov+1/N*np.matmul(f,f.T)\n",
    "\n",
    "        #Eigen values\n",
    "        E,V = np.linalg.eig(Cov)\t\t\n",
    "        #Sort eigenvalues and vectors to descending order\n",
    "        idx = np.argsort(E)[::-1]\n",
    "        V = np.matrix(V[:,idx])\n",
    "        E = E[idx]\n",
    "\n",
    "        for k in range(ncomp):\t\t\t\t\t\t\n",
    "            s = np.matmul(V[:,k].T,centrd).T\t\t\t\n",
    "            try:\n",
    "                score = np.concatenate((score,s),axis=1)\n",
    "            except NameError:\n",
    "                score = s\n",
    "            p = V[:,k]\n",
    "            try:\n",
    "                pcomp = np.concatenate((pcomp,p),axis=1)\n",
    "            except NameError:\n",
    "                pcomp = p\n",
    "    else:\n",
    "        #PCA with SVD\n",
    "        u,s,v = np.linalg.svd(centrd,compute_uv=1)\n",
    "        pcomp = v[:,:ncomp]\n",
    "        # Save results\n",
    "        writer = pd.ExcelWriter(r'C:\\Users\\sarytky\\Desktop\\trials' + r'\\PCA_test.xlsx')\n",
    "        df1 = pd.DataFrame(centrd)\n",
    "        df1.to_excel(writer, sheet_name='dataAdjust')\n",
    "        df2 = pd.DataFrame(u)\n",
    "        df2.to_excel(writer, sheet_name='u')\n",
    "        df3 = pd.DataFrame(s)\n",
    "        df3.to_excel(writer, sheet_name='s')\n",
    "        df4 = pd.DataFrame(v)\n",
    "        df4.to_excel(writer, sheet_name='v')        \n",
    "        writer.save()\n",
    "        np.savetxt(r'C:\\Users\\sarytky\\Desktop\\trials' + '\\\\''dataAdjust_python.csv', centrd, delimiter=',')\n",
    "\n",
    "        score = np.matmul(u,s).T[:,1:ncomp]\n",
    "    return pcomp,score\n",
    "\n",
    "#Local grayscale standardization\n",
    "def localstandard(im,w1,w2,sigma1,sigma2):\n",
    "    #Centers grayscales with Gaussian weighted mean\n",
    "    #Gaussian kernels\n",
    "    kernel1 = Gauss2D(w1,sigma1)\n",
    "    kernel2 = Gauss2D(w2,sigma2)\n",
    "    #Blurring\n",
    "    blurred1 = scipy.ndimage.convolve(im,kernel1)\n",
    "    blurred2 = scipy.ndimage.convolve(im,kernel2)\n",
    "    #print(blurred1[11,:])\n",
    "    #Centering grayscale values\n",
    "    centered = im-blurred1\n",
    "    #Standardization\n",
    "    std = (scipy.ndimage.convolve(centered**2,kernel2))**0.5\n",
    "    new_im = centered/(std+1e-09)\n",
    "    return new_im\n",
    "\n",
    "#Gaussian kernel\n",
    "def Gauss2D(w,sigma):\n",
    "    #Generates 2d gaussian kernel\n",
    "    kernel = np.zeros((w,w))\n",
    "    #Constant for centering\n",
    "    r = (w-1)/2\n",
    "    for ii in range(w):\n",
    "        for jj in range(w):\n",
    "            x = -((ii-r)**2+(jj-r)**2)/(2*sigma**2)\n",
    "            kernel[ii,jj] = np.exp(x)\n",
    "    #Normalizing the kernel\n",
    "    kernel = 1/np.sum(kernel)*kernel\n",
    "    return kernel\n",
    "\n",
    "def loadbinary(path):\n",
    "    bytesarray = np.fromfile(path, dtype=np.int32) # read everything as int32\n",
    "    w = bytesarray[0]\n",
    "    l = int((bytesarray.size - 1) / w)\n",
    "    with open(path, \"rb\") as f: # open to read binary file\n",
    "        f.seek(4) # skip first integer (width)\n",
    "        features = np.zeros((w,l))\n",
    "        for i in range(w):\n",
    "            for j in range(l):\n",
    "                features[i, j] = struct.unpack('<i', f.read(4))[0]  # when reading byte by byte (struct), \n",
    "                                                                #data type can be defined with every byte\n",
    "        return features\n",
    "\n",
    "def writebinaryweights(path, ncomp, eigenvectors, singularvalues, weights):\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(struct.pack('<i', eigenvectors.shape[1])) # Width\n",
    "        f.write(struct.pack('<i', ncomp)) # Number of components\n",
    "        # Eigenvectors \n",
    "        for i in range(eigenvectors.shape[0]):\n",
    "            for j in range(eigenvectors.shape[1]):\n",
    "                f.write(struct.pack('<f', eigenvectors[i, j]))\n",
    "        # Singular values\n",
    "        for i in range(singularvalues.shape[0]):\n",
    "            f.write(struct.pack('<f', singularvalues[i]))\n",
    "        # Weights\n",
    "        for i in range(weights.shape[0]):\n",
    "            f.write(struct.pack('<f', weights[i]))            \n",
    "    return True\n",
    "\n",
    "def writebinaryimage(path, image, dtype):\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(struct.pack('<i', image.shape[0])) # Width\n",
    "        # Image values as float\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in range(image.shape[1]):\n",
    "                if dtype == 'float':\n",
    "                    f.write(struct.pack('<f', image[i, j]))\n",
    "                if dtype == 'double':\n",
    "                    f.write(struct.pack('<d', image[i, j]))\n",
    "                if dtype == 'int':\n",
    "                    f.write(struct.pack('<i', image[i, j]))                    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Start time\n",
    "start_time = time.time()\n",
    "#Image\n",
    "path = r'C:\\Users\\sarytky\\Desktop\\trials'\n",
    "#sample = r'\\13_R3L_2_CA4+__rec_Tra_Tar00000127.png'\n",
    "#sample = r'\\T2_DE3D_SAG_0067.png'\n",
    "#sample = r'\\34_R6TL_10__Tar0014.png'\n",
    "\n",
    "#impath = r'C:\\Users\\sarytky\\Desktop\\T2_DE3D_SAG_ISO_0005_png'\n",
    "impath = r'V:\\Tuomas\\PTASurfaceImages'\n",
    "savepath = r'C:\\Users\\sarytky\\Desktop\\trials\\LBP_results_python'\n",
    "\n",
    "# Load grades to array\n",
    "grades = pd.read_excel(r'C:\\Users\\sarytky\\Desktop\\trials' + r'\\PTAgreiditjanaytteet.xls', 'Sheet1')\n",
    "grades = pd.DataFrame(grades).values\n",
    "#print(grades[:,1])\n",
    "#print(grades[:,2])\n",
    "g = grades[:,2].astype('int')\n",
    "grades = grades[:,1:2]\n",
    "\n",
    "\n",
    "#image = cv2.imread(path + sample, cv2.IMREAD_GRAYSCALE)\n",
    "#print(path + sample)\n",
    "#plt.imshow(image)\n",
    "#plt.show()\n",
    "\n",
    "#Calculate MRELBP from dataset\n",
    "\n",
    "# Parameters\n",
    "dict = {'N':8, 'R':9,'r':3,'wc':5,'wr':(5,5)}\n",
    "mapping = getmapping(dict['N']) # mapping\n",
    "\n",
    "files = os.listdir(impath)\n",
    "files.sort()\n",
    "filelist = files\n",
    "#print(files)\n",
    "features = None # Reset feature array\n",
    "\n",
    "for k in range(len(files)):\n",
    "#for k in range(1):\n",
    "    #Load file\n",
    "    print('Processing: ' + files[k])\n",
    "    file = os.path.join(impath,files[k])\n",
    "    try:\n",
    "        file = sio.loadmat(file)\n",
    "        Mz = file['Mz']\n",
    "        sz = file['sz']\n",
    "    except NotImplementedError:\n",
    "        file = h5py.File(file)\n",
    "        Mz = file['Mz'][()]\n",
    "        sz = file['sz'][()]\n",
    "    \n",
    "    #Combine mean and sd images\n",
    "    image = Mz+sz\n",
    "    #Grayscale normalization\n",
    "    image = localstandard(image,23,5,5,1)\n",
    "    #print(image[0,:]) # Normalized image\n",
    "    # LBP\n",
    "    Chist,Lhist,Shist,Rhist, lbpIL, lbpIS, lbpIR = MRELBP(image,dict['N'],dict['R'],dict['r'],dict['wc'],dict['wr'])\n",
    "    f1 = Chist\n",
    "    f2 = maplbp(Lhist,mapping)\n",
    "    f3 = maplbp(Shist,mapping)\n",
    "    f4 = maplbp(Rhist,mapping)\n",
    "    #Concatenate features\n",
    "    f = np.concatenate((f1.T,f2.T,f3.T,f4.T),axis=0)\n",
    "    try:\n",
    "        features = np.concatenate((features,f),axis=1)\n",
    "    except ValueError:\n",
    "        features = f\n",
    "    # Save images\n",
    "    cv2.imwrite(savepath + '\\\\' + files[k] + '.png', lbpIL)\n",
    "    #print('Saved LBP image: ' + files[k])\n",
    "    #print(np.matrix(Chist))\n",
    "    # Display images\n",
    "    plt.imshow(lbpIS)\n",
    "    plt.show()\n",
    "    plt.imshow(lbpIL)\n",
    "    plt.show()\n",
    "    plt.imshow(lbpIR)\n",
    "    plt.show()\n",
    "\n",
    "##    \n",
    "## Single image LBP\n",
    "#Chist,Lhist,Shist,Rhist, lbpIL, lbpIS, lbpIR = MRELBP(image,dict['N'],dict['R'],dict['r'],dict['wc'],dict['wr'])    \n",
    "## Save images\n",
    "#cv2.imwrite(path + '\\LBPIL_python.png', lbpIL)\n",
    "#cv2.imwrite(path + '\\LBPIS_python.png', lbpIS)\n",
    "#cv2.imwrite(path + '\\LBPIR_python.png', lbpIR)    \n",
    "##\n",
    "\n",
    "#print(features.T[:,0])\n",
    "\n",
    "# Save features\n",
    "writer = pd.ExcelWriter(path + r'\\LBP_features_python.xlsx')\n",
    "df1 = pd.DataFrame(features)\n",
    "df1.to_excel(writer, sheet_name='LBP_features')\n",
    "writer.save()\n",
    "\n",
    "#PCA\n",
    "pca, score = ScikitPCA(features.T,10)\n",
    "#pca, score = PCA(features,10)\n",
    "print(score[0,:])\n",
    "\n",
    "#print(score.shape)\n",
    "pred1 = regress(score,g)\n",
    "pred2 = logreg(score,g>0)\n",
    "#pred2 = logreg(features.T,g>0)\n",
    "for p in range(len(pred1)):\n",
    "    if pred1[p]<0:\n",
    "        pred1[p] = 0\n",
    "    if pred1[p] > 3:\n",
    "        pred1[p]=3\n",
    "\n",
    "#Plotting the prediction\n",
    "a = g\n",
    "b = np.round(pred1).astype('int')\t\n",
    "\n",
    "#Plotting\n",
    "x = score[:,0]\n",
    "y = score[:,1]\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "#plt.grid(True)\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(score[g<2,0],score[g<2,1],marker='o',color='b',label='Normal')\t\n",
    "ax1.scatter(score[g>1,0],score[g>1,1],marker='s',color='r',label='OA')\n",
    "\n",
    "for k in range(len(grades[:,0])):\n",
    "    txt = filelist[k]\n",
    "    txt = txt[0:-4]\n",
    "    txt = txt+str(grades[k,0])\t\t\n",
    "    if grades[k,0] >= 2:\n",
    "        ax1.scatter(x[k],y[k],marker='s',color='r')\n",
    "        #ax1.annotate(txt,xy=(x[k],y[k]),color='r')\n",
    "    else:\n",
    "        ax1.scatter(x[k],y[k],marker='o',color='b')\n",
    "        #ax1.annotate(txt,xy=(x[k],y[k]),color='b')\t\n",
    "\n",
    "C1 = skmet.confusion_matrix(a,b)\n",
    "MSE1 = skmet.mean_squared_error(a,pred1)\n",
    "fpr, tpr, thresholds = skmet.roc_curve(a>0, np.round(pred1)>0, pos_label=1)\n",
    "AUC1 = skmet.auc(fpr,tpr)\n",
    "AUC1 = skmet.roc_auc_score(a>0,pred2)\n",
    "t = time.time()-start_time\n",
    "m, b = np.polyfit(a, pred1.flatten(), 1)\n",
    "R2 = skmet.r2_score(a,pred1.flatten())\n",
    "fig0  = plt.figure(figsize=(6,6))\n",
    "ax0 = fig0.add_subplot(111)\n",
    "ax0.plot(fpr,tpr)\n",
    "\n",
    "print('Confusion matrix')\n",
    "print(C1)\n",
    "print('Mean squared error, Area under curve')\n",
    "print(MSE1,AUC1)#,MSE2,MSE3,MSE4)\n",
    "print(\"-- %s seconds --\" % t)\n",
    "print('R2 score')\n",
    "print(R2)\n",
    "print('Sample, grade, prediction')\n",
    "for k in range(len(filelist)):\n",
    "    print(filelist[k],a[k],pred1[k])#,pred3[k])\n",
    "    \n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax2 = fig.add_subplot(111)\n",
    "ax2.scatter(a,pred1.flatten())\n",
    "ax2.plot(a,m*a,'-',color='r')\n",
    "ax2.set_xlabel('Actual grade')\n",
    "ax2.set_ylabel('Predicted')\n",
    "for k in range(len(grades[:,0])):\n",
    "    txt = filelist[k]\n",
    "    txt = txt[0:-4]\n",
    "    txt = txt+str(grades[k,0])\n",
    "    ax2.annotate(txt,xy=(a[k],pred1[k]),color='r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22_L6TL_topo.mat\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'writebinaryimage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9db454020a37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilelist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilelist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mwritebinaryimage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msavepath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_mean.dat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mwritebinaryimage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msavepath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_std.dat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'writebinaryimage' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert .mat arrays to binary files\n",
    "path = r'V:\\Tuomas\\PTASurfaceImages'\n",
    "savepath = r'V:\\Tuomas\\PTASurfaceImages_binary'\n",
    "filelist = os.listdir(path)\n",
    "for k in range(len(filelist)):\n",
    "    #Load file\n",
    "    file = os.path.join(path,filelist[k])\n",
    "    try:\n",
    "        file = sio.loadmat(file)\n",
    "        Mz = file['Mz']\n",
    "        sz = file['sz']\n",
    "    except NotImplementedError:\n",
    "        file = h5py.File(file)\n",
    "        Mz = file['Mz'][()]\n",
    "        sz = file['sz'][()]\n",
    "        \n",
    "    # Save file\n",
    "    dtype = 'double'\n",
    "    Mz = np.float64(Mz)\n",
    "    sz = np.float64(sz)\n",
    "    name = filelist[k]\n",
    "    print(filelist[k])\n",
    "    writebinaryimage(savepath + '\\\\' + name[:-4] + '_mean.dat', Mz, dtype)\n",
    "    writebinaryimage(savepath + '\\\\' + name[:-4] + '_std.dat', sz, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22_L6TL_topo.mat\n",
      "O19_L6L_topo.mat\n",
      "O18_R2M_topo.mat\n",
      "13_R3L_topo.mat\n",
      "23_R6LT_topo.mat\n",
      "26_L3L_topo.mat\n",
      "O17_R6L_topo.mat\n",
      "14_R3L_topo.mat\n",
      "26_L6LT_topo.mat\n",
      "O19_L3L_topo.mat\n",
      "25_L6TL_topo.mat\n",
      "29_R6TM_topo.mat\n",
      "20_R6TM_topo.mat\n",
      "28_L3L_topo.mat\n",
      "31_R6LT_topo.mat\n",
      "15_L3L_topo.mat\n",
      "13_R6TL_topo.mat\n",
      "27R6T_topo.mat\n",
      "14_R6LT_topo.mat\n",
      "O17_R3L_topo.mat\n",
      "24_R6LT_topo.mat\n",
      "29_R2M_topo.mat\n",
      "32_L6MT_topo.mat\n",
      "30_R6TL_topo.mat\n",
      "22_L3L_topo.mat\n",
      "21_L3L_topo.mat\n",
      "27R3M_topo.mat\n",
      "32_L3L_topo.mat\n",
      "25_L3L_topo.mat\n",
      "28_L6MT_topo.mat\n",
      "21_L6LT_topo.mat\n",
      "24_R3L_topo.mat\n",
      "20_R2M_topo.mat\n",
      "23_R3L_topo.mat\n",
      "30_R3L_topo.mat\n",
      "15_L6TL_topo.mat\n"
     ]
    }
   ],
   "source": [
    "# Convert .mat arrays to .png files\n",
    "path = r'V:\\Tuomas\\PTASurfaceImages'\n",
    "savepath = r'V:\\Tuomas\\PTASurfaceImages_png'\n",
    "filelist = os.listdir(path)\n",
    "for k in range(len(filelist)):\n",
    "    #Load file\n",
    "    file = os.path.join(path,filelist[k])\n",
    "    try:\n",
    "        file = sio.loadmat(file)\n",
    "        Mz = file['Mz']\n",
    "        sz = file['sz']\n",
    "    except NotImplementedError:\n",
    "        file = h5py.File(file)\n",
    "        Mz = file['Mz'][()]\n",
    "        sz = file['sz'][()]\n",
    "        \n",
    "    # Save file\n",
    "    dtype = 'double'\n",
    "    mx = np.amax(np.float64(Mz))\n",
    "    mn = np.amin(np.float64(Mz))\n",
    "    Mbmp = (np.float64(Mz) - mn) * (255 / (mx - mn))\n",
    "    sx = np.amax(np.float64(sz))\n",
    "    sn = np.amin(np.float64(sz))\n",
    "    sbmp = (np.float64(sz) - sn) * (255 / (sx - sn))\n",
    "    name = filelist[k]\n",
    "    print(filelist[k])\n",
    "    #print(savepath + '\\\\' + name[:-4] +'_mean.png')\n",
    "    cv2.imwrite(savepath + '\\\\' + name[:-4] +'_mean.png', Mbmp)\n",
    "    cv2.imwrite(savepath + '\\\\' + name[:-4] +'_std.png', sbmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 2. 2. 2.]\n",
      " [1. 1. 1. 2. 2. 2.]\n",
      " [1. 1. 1. 2. 2. 2.]\n",
      " [3. 3. 3. 4. 4. 4.]\n",
      " [3. 3. 3. 4. 4. 4.]\n",
      " [3. 3. 3. 4. 4. 4.]]\n",
      "[[0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 2. 2. 0.]\n",
      " [1. 1. 2. 2. 2. 2.]\n",
      " [1. 2. 3. 3. 2. 2.]\n",
      " [0. 2. 3. 3. 2. 0.]\n",
      " [0. 0. 3. 3. 0. 0.]]\n",
      "<class 'numpy.float32'>\n",
      "100.00001\n"
     ]
    }
   ],
   "source": [
    "test = np.zeros((6,6))\n",
    "kernel = np.zeros((9,9))\n",
    "kernel = kernel + 1\n",
    "test[0:3,0:3] = 1\n",
    "test[3:6,0:3] = 3\n",
    "test[0:3,3:6] = 2\n",
    "test[3:6,3:6] = 4\n",
    "print(test)\n",
    "\n",
    "result = scipy.signal.medfilt(test,5)\n",
    "print(result)\n",
    "\n",
    "w = np.float32(100.00001)\n",
    "print(type(w))\n",
    "print(np.float32(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 2. 2. 2. 2.]\n",
      " [1. 1. 1. 1. 2. 2. 2. 2.]\n",
      " [1. 1. 1. 1. 2. 2. 2. 2.]\n",
      " [1. 1. 1. 1. 2. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 4. 4. 4. 4.]\n",
      " [3. 3. 3. 3. 4. 4. 4. 4.]\n",
      " [3. 3. 3. 3. 4. 4. 4. 4.]\n",
      " [3. 3. 3. 3. 4. 4. 4. 4.]]\n",
      "[[1.  1.  1.  1.5 2.  2. ]\n",
      " [1.  1.  1.  1.5 2.  2. ]\n",
      " [1.  1.  1.  1.5 2.  2. ]\n",
      " [2.  2.  2.  2.5 3.  3. ]\n",
      " [3.  3.  3.  3.5 4.  4. ]\n",
      " [3.  3.  3.  3.5 4.  4. ]]\n"
     ]
    }
   ],
   "source": [
    "test = np.zeros((8,8))\n",
    "test[0:4,0:4] = 1\n",
    "test[4:8,0:4] = 3\n",
    "test[0:4,4:8] = 2\n",
    "test[4:8,4:8] = 4\n",
    "print(test)\n",
    "x = 0.5\n",
    "y = 0.5\n",
    "col = 6\n",
    "row = 6\n",
    "interpolated = imbilinear(test,col,x,row,y)\n",
    "print(interpolated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'progress'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d3e9f7fd3439>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'progress'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
