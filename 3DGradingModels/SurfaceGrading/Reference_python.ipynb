{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import pandas as pd\n",
    "import struct\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from joblib import Parallel, delayed\n",
    "import sklearn.metrics as skmet\n",
    "import sklearn.linear_model as sklin\n",
    "\n",
    "\n",
    "import scipy.ndimage\n",
    "import sklearn.decomposition as skdec\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Bilinear interpolation\n",
    "\n",
    "#def imbilinear(im,col,x,row,y):\n",
    "#\t#Takes bilinear interpotalion from image\n",
    "#\t#Starts from coordinates [y,x], ends at row,col\n",
    "#\te = 1e-12\n",
    "#\tx1 = int(np.floor(x))\n",
    "#\tx2 = int(np.ceil(x))\n",
    "#\ty1 = int(np.floor(y))\n",
    "#\ty2 = int(np.ceil(y))\n",
    "#\tQ11 = im[y2:y2+row,x1:x1+col]\n",
    "#\tQ21 = im[y2:y2+row,x2:x2+col]\n",
    "#\tQ12 = im[y1:y1+row,x1:x1+col]\n",
    "#\tQ22 = im[y1:y1+row,x2:x2+col]\n",
    "#\tR1 = ((x2-x)/(x2-x1+e))*Q11+((x-x1)/(x2-x1+e))*Q21\n",
    "#\tR2 = ((x2-x)/(x2-x1+e))*Q12+((x-x1)/(x2-x1+e))*Q22\n",
    "#\tP = ((y2-y)/(y2-y1+e))*R1+((y-y1)/(y2-y1+e))*R2\n",
    "#\treturn P\n",
    "\n",
    "def imbilinear(im,col,x,row,y):\n",
    "    #Takes bilinear interpotalion from image\n",
    "    #Starts from coordinates [y,x], ends at row,col\n",
    "    e = 1e-12\n",
    "    x1 = int(x)\n",
    "    x2 = int(np.ceil(x))\n",
    "    y1 = int(y)\n",
    "    y2 = int(np.ceil(y))\n",
    "    Q11 = im[y1:y1+row,x1:x1+col]\n",
    "    Q21 = im[y1:y1+row,x2:x2+col]\n",
    "    Q12 = im[y2:y2+row,x1:x1+col]\n",
    "    Q22 = im[y2:y2+row,x2:x2+col]\n",
    "    R1 = ((x2-x)/(x2-x1+e))*Q11+((x-x1)/(x2-x1+e))*Q21\n",
    "    R2 = ((x2-x)/(x2-x1+e))*Q12+((x-x1)/(x2-x1+e))*Q22\n",
    "    P = ((y2-y)/(y2-y1+e))*R1+((y-y1)/(y2-y1+e))*R2\n",
    "    return P\n",
    "\n",
    "#Gaussian kernel\n",
    "def Gauss2D(w,sigma):\n",
    "    #Generates 2d gaussian kernel\n",
    "    kernel = np.zeros((w,w))\n",
    "    #Constant for centering\n",
    "    r = (w-1)/2\n",
    "    for ii in range(w):\n",
    "        for jj in range(w):\n",
    "            x = -((ii-r)**2+(jj-r)**2)/(2*sigma**2)\n",
    "            kernel[ii,jj] = np.exp(x)\n",
    "    #Normalizing the kernel\n",
    "    kernel = 1/np.sum(kernel)*kernel\n",
    "    return kernel\n",
    "\n",
    "#LBP\n",
    "def lbp(im,r,n):\n",
    "    #Takes lbp from numpy array\n",
    "    #Uses n neighbours from radius r\n",
    "\n",
    "    #center pixels\n",
    "    center = im[r:-r,r:-r]\n",
    "    row,col = np.shape(center)\n",
    "    #Differences\t\n",
    "\n",
    "    #Empty array for pattern\n",
    "    pattern = np.zeros((row,col))\n",
    "\n",
    "    #LBP\n",
    "    pi = np.pi\t\n",
    "    for k in range(n):\n",
    "        #print(k)\n",
    "        diff = np.empty((x,y))\n",
    "        theta = 0+k*(1*2*pi/n)\n",
    "        x = r+r*np.cos(theta)\n",
    "        y = r+r*np.sin(theta)\t\t\n",
    "        #Check if bilinear interpolation is needed\n",
    "        if abs(x-round(x))<1e-06 and abs(y-round(y))<1e-06:\n",
    "            x = int(round(x))\n",
    "            y = int(round(y))\n",
    "            P = im[y:y+row,x:x+col]\t\n",
    "        else:\t\t\t\n",
    "            P = imbilinear(im,col,x,row,y)\n",
    "        #Difference between neighbour and center pixel\t\t\n",
    "        diff = P[0:y,0:x] - center\n",
    "        pattern = pattern+(diff>=0)*2**(k*(diff>=0))\t\n",
    "\n",
    "        #Empty histogram\n",
    "        hist = np.zeros((1,2**n))\n",
    "        #Binning\n",
    "        for k in range(2**n):\n",
    "            hist[0,k] = np.sum(pattern==k)\n",
    "        hist = 1/np.linalg.norm(hist)*hist\n",
    "        return hist\n",
    "\n",
    "#MRELBP\n",
    "def MRELBP(im,N,R,r,w_c,w_r):\n",
    "    #Takes Median Robust Extended Local Binary Pattern from image im\n",
    "    #Uses N neighbours from radii R and r, R must be larger than r\n",
    "    #Median filter uses kernel sizes w_c for center pixels, w_r[0] for larger radius and w_r[1]\n",
    "    #for smaller radius\t\n",
    "    #Grayscale values are centered at their mean and scales with global standad deviation\n",
    "\n",
    "    #Mean grayscale value and std\n",
    "    muI = im.mean()\n",
    "    stdI = im.std()\n",
    "\n",
    "    #Centering and scaling with std\n",
    "    I = (im-muI)/stdI\n",
    "\n",
    "    #Median filtering\n",
    "    Ic = scipy.signal.medfilt(I,w_c)\t\n",
    "    #Center pixels\n",
    "    d = round(R+(w_r[0]-1)/2)\n",
    "    Ic = Ic[d:-d,d:-d]\n",
    "    #Subtracting the mean pixel value from center pixels\n",
    "    Ic = Ic-Ic.mean()\n",
    "    #Bining center pixels\n",
    "    Chist = np.zeros((1,2))\n",
    "    #Chist[0,0] = np.sum(Ic>=0)\n",
    "    #Chist[0,1] = np.sum(Ic<0)\t\n",
    "    Chist[0,0] = np.sum(Ic>=-1e-06)\n",
    "    Chist[0,1] = np.sum(Ic<-1e-06)\t\n",
    "\n",
    "    #Median filtered images for large and small radius\n",
    "    IL = scipy.signal.medfilt(I,w_r[0])\n",
    "    d1 = round((w_r[0]-1)/2)\n",
    "    IL = IL[d1:-d1,d1:-d1]\n",
    "    IS = scipy.signal.medfilt2d(I,w_r[1])\n",
    "    d2 = round((w_r[1]-1)/2)\n",
    "    IS = IS[d2:-d2,d2:-d2]\n",
    "\n",
    "    #Neighbours\n",
    "    pi = np.pi\n",
    "    #Empty arrays for the neighbours\n",
    "    row,col = np.shape(Ic)\n",
    "    NL = np.zeros((row,col,N))\n",
    "    NS = np.zeros((row,col,N))\n",
    "    for k in range(N):\n",
    "        #Angle to the neighbour\n",
    "        theta = 0+k*(-1*2*pi/N)\n",
    "        #Large neighbourhood\n",
    "        x = R+R*np.cos(theta)\n",
    "        y = R+R*np.sin(theta)\n",
    "        if abs(x-round(x)) < 1e-06 and abs(y-round(y)) < 1e-06:\n",
    "            x = int(round(x))\n",
    "            y = int(round(y))\n",
    "            P = IL[y:y+row,x:x+col]\n",
    "        else:\n",
    "            P = imbilinear(IL,col,x,row,y)\n",
    "        NL[:,:,k] = P\n",
    "        #Small neighbourhood\n",
    "        #x = r+r*np.cos(theta)\n",
    "        #y = r+r*np.sin(theta)\n",
    "        x = R+r*np.cos(theta)\n",
    "        y = R+r*np.sin(theta)        \n",
    "        if abs(x-round(x)) < 1e-06 and abs(y-round(y)) < 1e-06:\n",
    "            x = int(round(x))\n",
    "            y = int(round(y))\n",
    "            P = IS[y:y+row,x:x+col]\n",
    "        else:\n",
    "            P = imbilinear(IS,col,x,row,y)\n",
    "        NS[:,:,k] = P\n",
    "    #Thresholding\n",
    "\n",
    "    #Thresholding radial neighbourhood\n",
    "    NR = NL-NS\n",
    "\n",
    "    #Subtraction of means\n",
    "    #Large neighbourhood\n",
    "    NLmu = NL.mean(axis=2)\t\t\n",
    "    #Small neighbouhood\n",
    "    NSmu = NS.mean(axis=2)\n",
    "\n",
    "    for k in range(N):\n",
    "        NL[:,:,k] = NL[:,:,k]-NLmu\n",
    "        NS[:,:,k] = NS[:,:,k]-NSmu\t\n",
    "\n",
    "    #Converting to binary images and taking the lbp values\n",
    "\n",
    "    #Initialization of arrays\n",
    "    lbpIL = np.zeros((row,col))\n",
    "    lbpIS = np.zeros((row,col))\n",
    "    lbpIR = np.zeros((row,col))\n",
    "\n",
    "    for k in range(N):\n",
    "        #lbpIL = lbpIL+(NL[:,:,k]>=0)*2**(k*(NL[:,:,k]>=0))\n",
    "        #lbpIS = lbpIS+(NS[:,:,k]>=0)*2**(k*(NS[:,:,k]>=0))\n",
    "        #lbpIR = lbpIR+(NR[:,:,k]>=0)*2**(k*(NR[:,:,k]>=0))\n",
    "        lbpIL = lbpIL+(NL[:,:,k]>=-1e-06)*2**(k)#*(NL[:,:,k]>=0))\n",
    "        lbpIS = lbpIS+(NS[:,:,k]>=-1e-06)*2**(k)#*(NS[:,:,k]>=0))\n",
    "        lbpIR = lbpIR+(NR[:,:,k]>=-1e-06)*2**(k)#*(NR[:,:,k]>=0))        \n",
    "\n",
    "    #Binning\n",
    "    Lhist = np.zeros((1,2**N))\n",
    "    Shist = np.zeros((1,2**N))\n",
    "    Rhist = np.zeros((1,2**N))\n",
    "    for k in range(2**N):\n",
    "        Lhist[0,k] = np.sum(lbpIL==k)\n",
    "        Shist[0,k] = np.sum(lbpIS==k)\n",
    "        Rhist[0,k] = np.sum(lbpIR==k)\n",
    "\n",
    "    #Chist = 1/np.linalg.norm(Chist)*Chist\n",
    "    #Lhist = 1/np.linalg.norm(Lhist)*Lhist\n",
    "    #Shist = 1/np.linalg.norm(Shist)*Shist\n",
    "    #Rhist = 1/np.linalg.norm(Rhist)*Rhist\n",
    "    return Chist,Lhist,Shist,Rhist\n",
    "\n",
    "#Mapping\t\n",
    "def getmapping(N):\n",
    "    #Defines rotation invariant uniform mapping for lbp of N neighbours\t\n",
    "    newMax = N + 2\n",
    "    table = np.zeros((1,2**N))\n",
    "    for k in range(2**N):\n",
    "        #Binary representation of bin number\n",
    "        binrep = np.binary_repr(k,N)\n",
    "        #Convert string to list of digits\n",
    "        i_bin = np.zeros((1,len(binrep)))\n",
    "        for ii in range(len(binrep)):\n",
    "            i_bin[0,ii] = int(float(binrep[ii]))\n",
    "        #Rotation\n",
    "        j_bin = np.roll(i_bin,-1)\n",
    "        #uniformity\n",
    "        numt = np.sum(i_bin!=j_bin)\t\t\n",
    "        #Binning\n",
    "        if numt <= 2:\n",
    "            b = np.binary_repr(k,N)\n",
    "            c=0\n",
    "            for ii in range(len(b)):\n",
    "                c = c+int(float(b[ii]))\n",
    "            table[0,k] = c\n",
    "        else:\n",
    "            table[0,k] = N+1\n",
    "    #num = newMax\n",
    "    return table\n",
    "\n",
    "#Apply mapping to lbp\n",
    "def maplbp(bin,mapping):\n",
    "    #Applies mapping to lbp bin\n",
    "    #Number of bins in output\n",
    "    N = int(np.max(mapping))\n",
    "    #Empty array\n",
    "    outbin = np.zeros((1,N+1))\n",
    "    for k in range(N+1):\n",
    "        #RIU indices\n",
    "        M = mapping==k\n",
    "        #Extract indices from original bin to new bin\n",
    "        outbin[0,k] = np.sum(M*bin)\n",
    "    return outbin\n",
    "\n",
    "#Scikit PCA\n",
    "def ScikitPCA(features,ncomp):\n",
    "    pca = skdec.PCA(n_components=ncomp)\n",
    "    score = pca.fit(features.T).transform(features.T)\n",
    "    return score\n",
    "\n",
    "#Principal component analysis\n",
    "def PCA(features,ncomp):\t\n",
    "    #Feature dimension, x=num variables,N=num observations\n",
    "    x,N = np.shape(features)\t\t\n",
    "    #Mean feature\n",
    "    mean_f = np.mean(features,axis=1)\n",
    "    #Centering\n",
    "    centrd = np.zeros((x,N))\n",
    "    for k in range(N):\n",
    "        centrd[:,k] = features[:,k]-mean_f\n",
    "\n",
    "    #PCs from covariance matrix if N>=x, svd otherwise\n",
    "    if x<=N:\n",
    "        #Covariance matrix\n",
    "        Cov = np.zeros((x,x))\n",
    "        f = np.zeros((x,1))\n",
    "        for k in range(N):\t\t\n",
    "            f[:,0] = centrd[:,k]\n",
    "            Cov = Cov+1/N*np.matmul(f,f.T)\n",
    "\n",
    "        #Eigen values\n",
    "        E,V = np.linalg.eig(Cov)\t\t\n",
    "        #Sort eigenvalues and vectors to descending order\n",
    "        idx = np.argsort(E)[::-1]\n",
    "        V = np.matrix(V[:,idx])\n",
    "        E = E[idx]\n",
    "\n",
    "        for k in range(ncomp):\t\t\t\t\t\t\n",
    "            s = np.matmul(V[:,k].T,centrd).T\t\t\t\n",
    "            try:\n",
    "                score = np.concatenate((score,s),axis=1)\n",
    "            except NameError:\n",
    "                score = s\n",
    "            p = V[:,k]\n",
    "            try:\n",
    "                pcomp = np.concatenate((pcomp,p),axis=1)\n",
    "            except NameError:\n",
    "                pcomp = p\n",
    "    else:\n",
    "        #PCA with SVD\n",
    "        u,s,v = np.linalg.svd(centrd,compute_uv=1)\n",
    "        pcomp = v[:,:ncomp]\n",
    "        score = np.matmul(u,s).T[:,1:ncomp]\t\t\n",
    "    return pcomp,score\n",
    "\n",
    "#Adaptive grayscale centering\n",
    "def AGCentering(im,w):\n",
    "    #Takes mean value in the window and subtracts it from the pixels within the window\n",
    "    #Scales grayscale values to range [0,1]\n",
    "    #w must be odd\n",
    "\n",
    "    #Image size\n",
    "    row,col = np.shape(im)\n",
    "    #Empty image for output\n",
    "    new_im = np.zeros((row,col))\n",
    "    r = int((w-1)/2)\n",
    "    for ii in range(row-2*r):\n",
    "        for jj in range(col-2*r):\n",
    "            y = r+1+ii\t\t\t\n",
    "            x = r+1+jj\t\t\t\n",
    "            new_im[y-r:y+r,x-r:x+r] = im[y-r:y+r,x-r:x+r]-np.mean(im[y-r:y+r,x-r:x+r])\n",
    "\n",
    "    #Scales grayscale values to range[0 1]\n",
    "    Gmax = np.max(new_im)\n",
    "    Gmin = np.min(new_im)\n",
    "    new_im = (new_im-Gmin)/(Gmax-Gmin)\n",
    "    return new_im\n",
    "\n",
    "#Local grayscale normalization normalization\n",
    "def localnorm(im,w,sigma):\n",
    "    #Centers grayscales with Gaussian weighted mean\n",
    "    #Gaussian kernel\t\n",
    "    kernel = Gauss2D(w,sigma)\n",
    "    #Blurring\n",
    "    blurred = scipy.ndimage.convolve(im,kernel)\n",
    "    #Normalizing with the blurred image\n",
    "    new_im = im-blurred\n",
    "    return new_im\n",
    "\n",
    "#Local grayscale standardization\n",
    "def localstandard(im,w1,w2,sigma1,sigma2):\n",
    "    #Centers grayscales with Gaussian weighted mean\n",
    "    #Gaussian kernels\n",
    "    kernel1 = Gauss2D(w1,sigma1)\n",
    "    kernel2 = Gauss2D(w2,sigma2)\t\t\n",
    "    #Blurring\n",
    "    blurred1 = scipy.ndimage.convolve(im,kernel1)\n",
    "    blurred2 = scipy.ndimage.convolve(im,kernel2)\n",
    "    #Centering grayscale values\n",
    "    centered = im-blurred1\n",
    "    #Standardization\n",
    "    std = (scipy.ndimage.convolve(centered**2,kernel2))**0.5\n",
    "    new_im = centered/(std+1e-09)\n",
    "    return new_im\n",
    "\n",
    "#Subimage generation\n",
    "def subimage(im,w,num):\n",
    "    #Splits the image in num*num subimage of size 2w x 2w\n",
    "    #Image size\n",
    "    row,col = np.shape(im)\n",
    "    imageset = np.zeros((2*w,2*w,num**2))\n",
    "    #Image counter\n",
    "    c = 0\n",
    "    for ky in range(num):\n",
    "        for kx in range(num):\n",
    "            stepx = int((col-2*w)/(num-1))\n",
    "            x1 = kx*stepx\n",
    "            x2 = x1+2*w\n",
    "            stepy = int((row-2*w)/(num-1))\n",
    "            y1 = ky*stepy\n",
    "            y2 = y1+2*w\n",
    "            imageset[:,:,c] = im[y1:y2,x1:x2]\n",
    "            c=c+1\n",
    "    return imageset\n",
    "\n",
    "#Regression\n",
    "def regress(features,score):\n",
    "    pred = []\n",
    "    #Leave one out split\n",
    "    loo = LeaveOneOut()\t\n",
    "    for trainidx, testidx in loo.split(features):\n",
    "        #Indices\n",
    "        X_train, X_test = features[trainidx], features[testidx]\n",
    "        X_test -= X_train.mean(0)\n",
    "        X_train -= X_train.mean(0)\n",
    "\n",
    "        Y_train, Y_test = score[trainidx], score[testidx]\t\t\n",
    "        #Linear regression\t\t\n",
    "        regr = sklin.Ridge(alpha=1)\n",
    "        regr.fit(X_train,Y_train)\n",
    "        #Predicted score\t\t\n",
    "        pred.append(regr.predict(X_test))\n",
    "\n",
    "    return np.array(pred)\n",
    "\n",
    "\n",
    "#Logistic regression\n",
    "def logreg(features,score):\n",
    "    pred = []\n",
    "    #Leave one out split\n",
    "    loo = LeaveOneOut()\t\n",
    "    for trainidx, testidx in loo.split(features):\n",
    "        #Indices\n",
    "        X_train, X_test = features[trainidx], features[testidx]\n",
    "        X_test -= X_train.mean(0)\n",
    "        X_train -= X_train.mean(0)\n",
    "\n",
    "        Y_train, Y_test = score[trainidx], score[testidx]\t\t\n",
    "        #Linear regression\n",
    "        regr = sklin.LogisticRegression(solver='newton-cg',max_iter=1000)\n",
    "        regr.fit(X_train,Y_train)\n",
    "        #Predicted score\n",
    "        P = regr.predict_proba(X_test)\n",
    "        pred.append(P)\n",
    "\n",
    "    pred = np.array(pred)\n",
    "    pred = pred[:,:,1]\n",
    "    return pred.flatten()\n",
    "#Linear regression with LOO and KFOLD\n",
    "def KFregress(features,score,fold):\n",
    "    row,col = np.shape(features)\n",
    "    fmean = np.mean(features,axis=1)\n",
    "    for k in range(col):\n",
    "        features[:,k] = features[:,k] - fmean.T\n",
    "    pred = np.zeros((row,1))\n",
    "    #Leave one out split\n",
    "    loo = LeaveOneOut()\t\n",
    "    for trainidx, testidx in loo.split(features):\n",
    "        #Indices\n",
    "        X_train, X_test = features[trainidx], features[testidx]\n",
    "        Y_train, Y_test = score[trainidx], score[testidx]\n",
    "        #KFold split\n",
    "        kpred = np.zeros((int(len(X_train)/fold),1))\n",
    "        kf = KFold(n_splits = fold)\n",
    "        c=0\n",
    "        for ktrain,ktest in kf.split(X_train):\n",
    "            X_Ktrain = X_train[ktrain]\n",
    "            Y_Ktrain = Y_train[ktrain]\n",
    "            #Linear regression\n",
    "            regr = sklin.LinearRegression()\n",
    "            regr.fit(X_Ktrain,Y_Ktrain)\n",
    "            #Predicted score\t\t\n",
    "            kpred[c,0] = regr.predict(X_test)\n",
    "            c=c+1\n",
    "        pred[testidx,0] = np.mean(kpred,axis=0)\n",
    "    return pred\n",
    "\n",
    "'''\n",
    "#LDA\n",
    "def LDA(features,score):\t\n",
    "    row,col = np.shape(features)\n",
    "    fmean = np.mean(features,axis=1)\n",
    "    for k in range(col):\n",
    "        features[:,k] = features[:,k] - fmean.T\n",
    "    pred = np.zeros((row,1))\n",
    "    #Leave one out split\n",
    "    loo = LeaveOneOut()\t\n",
    "    for trainidx, testidx in loo.split(features):\n",
    "        #Indices\n",
    "        X_train, X_test = features[trainidx], features[testidx]\n",
    "        Y_train, Y_test = score[trainidx], score[testidx]\n",
    "        clf = LinearDiscriminantAnalysis()\n",
    "        clf.fit(X_train,Y_train)\n",
    "        pred[testidx,0] = clf.predict(X_test)\n",
    "    return pred\n",
    "'''\n",
    "\n",
    "#Random forest\n",
    "def RF(features,score,trees,d):\n",
    "    score = np.ravel(score)\n",
    "    row,col = np.shape(features)\t\n",
    "    pred = np.zeros((row,1))\n",
    "    #Leaveone out split\n",
    "    loo = LeaveOneOut()\n",
    "    for trainidx,testidx in loo.split(features):\n",
    "    #Indices\n",
    "        X_train, X_test = features[trainidx], features[testidx]\n",
    "        Y_train, Y_test = score[trainidx], score[testidx]\t\t\n",
    "        #Random Forest\n",
    "        rf = RandomForestClassifier(n_estimators=trees,n_jobs = 4,max_depth=d,random_state=42)\n",
    "        rf.fit(X_train,Y_train)\n",
    "        #Prediction\n",
    "        pred[testidx,0] = rf.predict(X_test)\n",
    "    return pred\n",
    "\n",
    "def augregression(testf,features,score):\n",
    "    pred = []\n",
    "    #Leave one out split\t\n",
    "    N = len(features[:,0])/len(testf[:,0])\n",
    "    for k in range(len(testf[:,0])):\n",
    "        #Indices and features\n",
    "        X_test = testf[k,:]\n",
    "        X_test = X_test.reshape(1,-1)\n",
    "        idx = np.linspace(0,N-1,N)+k*N\n",
    "        X_train = features\n",
    "        X_train = np.delete(X_train,idx,axis=0)\n",
    "        #Subtraction of mean\n",
    "        X_train -= X_train.mean(0)\n",
    "        X_test -= X_train.mean(0)\n",
    "        Y_train = score\n",
    "        Y_train = np.delete(Y_train,idx)\t\t\t\t\n",
    "        #Linear regression\n",
    "        regr = sklin.Ridge(alpha=1)\n",
    "        regr.fit(X_train,Y_train)\n",
    "        #Predicted score\t\t\n",
    "        pred.append(regr.predict(X_test))\n",
    "\n",
    "    return np.array(pred)\n",
    "\n",
    "def augRF(testf,features,score,trees,d):\n",
    "    pred = []\n",
    "    #Leave one out split\n",
    "    N = len(features[:,0])/len(testf[:,0])\t\n",
    "    for k in range(len(testf[:,0])):\n",
    "        #Indices and features\n",
    "        X_test = testf[k,:]\n",
    "        X_test = X_test.reshape(1,-1)\t\t\n",
    "        idx = np.linspace(0,N-1,N)+k*N\n",
    "        X_train = features\n",
    "        X_train = np.delete(X_train,idx,axis=0)\t\t\n",
    "        Y_train = score\n",
    "        Y_train = np.delete(Y_train,idx)\t\t\t\t\n",
    "        #Random forest regression\n",
    "        rf = RandomForestClassifier(n_estimators=trees,n_jobs = 4,max_depth=d,random_state=42)\n",
    "        rf.fit(X_train,Y_train)\n",
    "        #Predicted score\t\t\n",
    "        pred.append(rf.predict(X_test))\n",
    "\n",
    "    return np.array(pred)\n",
    "\n",
    "def SVM(features,score):\n",
    "    score = np.ravel(score)\n",
    "    row,col = np.shape(features)\t\n",
    "    pred = np.zeros((row,1))\n",
    "    #Leaveone out split\n",
    "    loo = LeaveOneOut()\n",
    "    for trainidx,testidx in loo.split(features):\n",
    "    #Indices\n",
    "        X_train, X_test = features[trainidx], features[testidx]\n",
    "        Y_train, Y_test = score[trainidx], score[testidx]\t\t\n",
    "        #Random Forest\n",
    "        clf = svm.SVC(random_state=42)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        #Prediction\n",
    "        pred[testidx,0] = clf.predict(X_test)\n",
    "    return pred\n",
    "\n",
    "def KNN(features,score):\n",
    "    score = np.ravel(score)\n",
    "    row,col = np.shape(features)\t\n",
    "    pred = np.zeros((row,1))\n",
    "    #Leaveone out split\n",
    "    loo = LeaveOneOut()\n",
    "    for trainidx,testidx in loo.split(features):\n",
    "    #Indices\n",
    "        X_train, X_test = features[trainidx], features[testidx]\n",
    "        Y_train, Y_test = score[trainidx], score[testidx]\t\t\n",
    "        #Random Forest\n",
    "        clf = neighbors.KNeighborsRegressor(n_neighbors=5,weights='distance')\n",
    "        clf.fit(X_train,Y_train)\n",
    "        #Prediction\n",
    "        pred[testidx,0] = clf.predict(X_test)\n",
    "    return pred\n",
    "\n",
    "def LDA(features,score):\n",
    "    score = np.ravel(score)\n",
    "    row,col = np.shape(features)\t\n",
    "    pred = np.zeros((row,1))\n",
    "    #Leaveone out split\n",
    "    loo = LeaveOneOut()\n",
    "    for trainidx,testidx in loo.split(features):\n",
    "    #Indices\n",
    "        X_train, X_test = features[trainidx], features[testidx]\n",
    "        Y_train, Y_test = score[trainidx], score[testidx]\t\t\n",
    "        #Random Forest\n",
    "        clf = LinearDiscriminantAnalysis()\n",
    "        clf.fit(X_train,Y_train)\n",
    "        #Prediction\n",
    "        pred[testidx,0] = clf.predict(X_test)\n",
    "    return pred\n",
    "def load_and_f(path,files):\n",
    "    #Mapping for lbp\n",
    "    mapping = getmapping(8)\n",
    "    for k in range(len(files)):\n",
    "        #Load file\n",
    "        file = os.path.join(path,files[k])\n",
    "        try:\n",
    "            file = sio.loadmat(file)\n",
    "            Mz = file['Mz']\n",
    "            sz = file['sz']\t\t\t\n",
    "        except NotImplementedError:\n",
    "            file = h5py.File(file)\n",
    "            Mz = file['Mz'][()]\n",
    "            sz = file['sz'][()]\t\t\t\n",
    "\n",
    "        #images\n",
    "\n",
    "        #Combine mean and sd images\n",
    "        image = Mz+sz\n",
    "        #Grayscale normalization\n",
    "        image = localstandard(image,23,5,5,1)\n",
    "        #image = image[20:-20,20:-20]\n",
    "        #Feature extraction\n",
    "        dict = {'R':9,'r':3,'wc':5,'wr':(5,5)}\t\t\n",
    "        f1,f2,f3,f4 = MRELBP(image,8,dict['R'],dict['r'],dict['wc'],dict['wr'])\n",
    "\n",
    "        #Normalization and mapping of the features f2(large neighbourhood lbp) and f4(radial lbp)\n",
    "\n",
    "        f2 = maplbp(f2,mapping)\n",
    "        f3 = maplbp(f3,mapping)\n",
    "        f4 = maplbp(f4,mapping)\n",
    "        #f1 = 1/np.linalg.norm(f1)*f1\n",
    "        #f2 = 1/np.linalg.norm(f2)*f2\n",
    "        #f3 = 1/np.linalg.norm(f3)*f3\n",
    "        #f4 = 1/np.linalg.norm(f4)*f4\n",
    "\n",
    "        #Concatenate features\n",
    "        f = np.concatenate((f1.T,f2.T,f3.T,f4.T),axis=0)\n",
    "        try:\n",
    "            features = np.concatenate((features,f),axis=1)\n",
    "        except NameError:\n",
    "            features = f\n",
    "\n",
    "    return features\n",
    "\n",
    "def parallel_f(path,files,n_jobs): \n",
    "    parallelizer = Parallel(n_jobs=n_jobs)\n",
    "    nlist = []\n",
    "    N = int(len(files)/n_jobs)\n",
    "    for k in range(n_jobs):\n",
    "        nlist.append(files[k*N:(k+1)*N])\n",
    "\n",
    "    iterator = ( delayed(load_and_f)(path,nfiles)\n",
    "                for nfiles in nlist )\n",
    "    result = parallelizer(iterator)\n",
    "    features = np.hstack(result)\n",
    "    return features, result\n",
    "\n",
    "def loadbinary(path):\n",
    "    bytesarray = np.fromfile(path, dtype=np.int32) # read everything as int32\n",
    "    w = bytesarray[0]\n",
    "    l = int((bytesarray.size - 1) / w)\n",
    "    with open(path, \"rb\") as f: # open to read binary file\n",
    "        f.seek(4) # skip first integer (width)\n",
    "        features = np.zeros((w,l))\n",
    "        for i in range(w):\n",
    "            for j in range(l):\n",
    "                features[i, j] = struct.unpack('<i', f.read(4))[0]  # when reading byte by byte (struct), \n",
    "                                                                #data type can be defined with every byte\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtracted features\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "Sum of differences\n",
      "0.0\n",
      "\n",
      "Relative difference\n",
      "0.0\n",
      "(31, 36)\n",
      "(35,)\n",
      "\n",
      "Sum of PCA differences\n",
      "179491.51953983074\n",
      "\n",
      "Relative PCA difference\n",
      "0.3235289514464\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 35 is out of bounds for axis 0 with size 35",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-eb89c2268f12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m#Regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mpred1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[0mpred2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m#pred2 = logreg(features.T,g>0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-d14a85b5ee6d>\u001b[0m in \u001b[0;36mregress\u001b[1;34m(features, score)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrainidx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestidx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m         \u001b[1;31m#Linear regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[0mregr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRidge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 35 is out of bounds for axis 0 with size 35"
     ]
    }
   ],
   "source": [
    "#Start time\n",
    "start_time = time.time()\n",
    "#Samples\n",
    "impath = r'V:\\Tuomas\\PTASurfaceImages'\n",
    "filelist = os.listdir(impath)\n",
    "filelist.sort()\n",
    "\n",
    "##Grades from excel file\n",
    "#grades = pd.ExcelFile(r'C:\\Users\\jfrondel\\Desktop\\Work\\Koodit\\BOFKoodia\\Segmentation\\PTAgreiditjanaytteet.xls')\n",
    "#grades = pd.read_excel(grades)\n",
    "#grades = pd.DataFrame.as_matrix(grades)\n",
    "#grades = grades[:,2:3]\n",
    "#g = grades[:,0].astype('int')\n",
    "\n",
    "# Load grades to array\n",
    "grades = pd.read_excel(r'C:\\Users\\sarytky\\Desktop\\trials' + r'\\PTAgreiditjanaytteet.xls', 'Sheet1')\n",
    "grades = pd.DataFrame(grades).values\n",
    "g = grades[:,2].astype('int')\n",
    "grades = grades[:,1:2]\n",
    "\n",
    "#Features\n",
    "features,result = parallel_f(impath,filelist,4)\n",
    "features_csharp = loadbinary(r'C:\\Users\\sarytky\\Desktop\\trials' + r'\\features.dat')\n",
    "print('Subtracted features')\n",
    "print(features_csharp-features)\n",
    "print('\\nSum of differences')\n",
    "print(np.sum(np.absolute(features_csharp-features)))\n",
    "print('\\nRelative difference')\n",
    "print(np.sum(np.absolute(features_csharp-features))/np.sum(features))\n",
    "\n",
    "#PCA\n",
    "score = ScikitPCA(features,10)\n",
    "score_csharp = ScikitPCA(features_csharp,10)\n",
    "print('\\nSum of PCA differences')\n",
    "print(np.sum(np.absolute(score_csharp-score)))\n",
    "print('\\nRelative PCA difference')\n",
    "print(np.sum(np.absolute(score_csharp-score))/np.sum(np.absolute(score)))\n",
    "#print(score.shape)\n",
    "\n",
    "#Regression\n",
    "pred1 = regress(score,g)\n",
    "pred2 = logreg(score,g>0)\n",
    "#pred2 = logreg(features.T,g>0)\n",
    "for p in range(len(pred1)):\n",
    "    if pred1[p]<0:\n",
    "        pred1[p] = 0\n",
    "    if pred1[p] > 3:\n",
    "        pred1[p]=3\n",
    "\n",
    "#Plotting the prediction\n",
    "a = g\n",
    "b = np.round(pred1).astype('int')\t\n",
    "\n",
    "#Plotting PCA scores\n",
    "x = score[:,0]\n",
    "y = score[:,1]\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(score[g<2,0],score[g<2,1],marker='o',color='b',label='Normal')\t\n",
    "ax1.scatter(score[g>1,0],score[g>1,1],marker='s',color='r',label='OA')\n",
    "\n",
    "#for k in range(len(grades[:,0])):\n",
    "#    txt = filelist[k]\n",
    "#    txt = txt[0:-4]\n",
    "#    txt = txt+str(grades[k,0])\t\t\n",
    "#    if grades[k,0] >= 2:\n",
    "#        ax1.scatter(x[k],y[k],marker='s',color='r')\n",
    "#        #ax1.annotate(txt,xy=(x[k],y[k]),color='r')\n",
    "#    else:\n",
    "#        ax1.scatter(x[k],y[k],marker='o',color='b')\n",
    "#        #ax1.annotate(txt,xy=(x[k],y[k]),color='b')\t\n",
    "print('\\nSample, grade, prediction')\n",
    "for k in range(len(filelist)):\n",
    "    print(filelist[k],a[k],pred1[k])#,pred3[k])\n",
    "\n",
    "C1 = skmet.confusion_matrix(a,b)\n",
    "MSE1 = skmet.mean_squared_error(a,pred1)\n",
    "fpr, tpr, thresholds = skmet.roc_curve(a>0, np.round(pred1)>0, pos_label=1)\n",
    "AUC1 = skmet.auc(fpr,tpr)\t\n",
    "AUC1 = skmet.roc_auc_score(a>0,pred2)\n",
    "t = time.time()-start_time\n",
    "m, b = np.polyfit(a, pred1.flatten(), 1)\n",
    "R2 = skmet.r2_score(a,pred1.flatten())\n",
    "\n",
    "# Area under curve\n",
    "fig0  = plt.figure(figsize=(6,6))\n",
    "ax0 = fig0.add_subplot(111)\n",
    "ax0.plot(fpr,tpr)\n",
    "\n",
    "print('Confusion matrix')\n",
    "print(C1)\n",
    "print('Mean squared error, Area under curve')\n",
    "print(MSE1,AUC1)#,MSE2,MSE3,MSE4)\n",
    "print(\"-- %s seconds --\" % t)\n",
    "print('R2 score')\n",
    "print(R2)\n",
    "\n",
    "print('Prediction: ')\n",
    "print(pred1)\n",
    "print('Difference:')\n",
    "print(pred1.flatten() - g)\n",
    "print('Sum of differences')\n",
    "print(np.sum(np.abs(pred1.flatten() - g)))\n",
    "\n",
    "# Grade vs. predicted\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax2 = fig.add_subplot(111)\n",
    "ax2.scatter(a,pred1.flatten())\n",
    "ax2.plot(a,m*a,'-',color='r')\n",
    "ax2.set_xlabel('Actual grade')\n",
    "ax2.set_ylabel('Predicted')\n",
    "for k in range(len(grades[:,0])):\n",
    "    txt = filelist[k]\n",
    "    txt = txt[0:-4]\n",
    "    txt = txt+str(grades[k,0])\n",
    "    ax2.annotate(txt,xy=(a[k],pred1[k]),color='r')\n",
    "plt.show()\n",
    "\n",
    "#Save everything\n",
    "dict = {'g':g,'pred1':pred1,'pred2':pred2}\n",
    "sio.savemat(r'C:\\Users\\sarytky\\Desktop\\trials\\regressresults_ref.mat',dict)\n",
    "r'''\n",
    "#Save everything to excel file\n",
    "Data = np.concatenate((g.flatten,pred1.flatten(),pred2.flatten()))\n",
    "df1 = pd.DataFrame(Data)\n",
    "writer = pd.ExcelWriter(r'c:\\users\\jfrondel\\desktop\\output.xlsx')\n",
    "df1.to_excel(writer)\n",
    "writer.save()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = np.zeros((8,8))\n",
    "test[0:4,0:4] = 1\n",
    "test[4:8,0:4] = 3\n",
    "test[0:4,4:8] = 2\n",
    "test[4:8,4:8] = 4\n",
    "print(test)\n",
    "x = 1.25\n",
    "y = 1.25\n",
    "col = 6\n",
    "row = 6\n",
    "interpolated = imbilinear(test,col,x,row,y)\n",
    "print(interpolated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "pred1 = regress(score_csharp,g)\n",
    "pred2 = logreg(score_csharp,g>0)\n",
    "#pred2 = logreg(features.T,g>0)\n",
    "for p in range(len(pred1)):\n",
    "    if pred1[p]<0:\n",
    "        pred1[p] = 0\n",
    "    if pred1[p] > 3:\n",
    "        pred1[p]=3\n",
    "\n",
    "#Plotting the prediction\n",
    "a = g\n",
    "b = np.round(pred1).astype('int')\t\n",
    "\n",
    "#Plotting PCA scores\n",
    "x = score_csharp[:,0]\n",
    "y = score_csharp[:,1]\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(score_csharp[g<2,0],score_csharp[g<2,1],marker='o',color='b',label='Normal')\t\n",
    "ax1.scatter(score_csharp[g>1,0],score_csharp[g>1,1],marker='s',color='r',label='OA')\n",
    "\n",
    "#for k in range(len(grades[:,0])):\n",
    "#    txt = filelist[k]\n",
    "#    txt = txt[0:-4]\n",
    "#    txt = txt+str(grades[k,0])\t\t\n",
    "#    if grades[k,0] >= 2:\n",
    "#        ax1.scatter(x[k],y[k],marker='s',color='r')\n",
    "#        #ax1.annotate(txt,xy=(x[k],y[k]),color='r')\n",
    "#    else:\n",
    "#        ax1.scatter(x[k],y[k],marker='o',color='b')\n",
    "#        #ax1.annotate(txt,xy=(x[k],y[k]),color='b')\t\n",
    "print('\\nSample, grade, prediction')\n",
    "for k in range(len(filelist)):\n",
    "    print(filelist[k],a[k],pred1[k])#,pred3[k])\n",
    "\n",
    "C1 = skmet.confusion_matrix(a,b)\n",
    "MSE1 = skmet.mean_squared_error(a,pred1)\n",
    "fpr, tpr, thresholds = skmet.roc_curve(a>0, np.round(pred1)>0, pos_label=1)\n",
    "AUC1 = skmet.auc(fpr,tpr)\t\n",
    "AUC1 = skmet.roc_auc_score(a>0,pred2)\n",
    "t = time.time()-start_time\n",
    "m, b = np.polyfit(a, pred1.flatten(), 1)\n",
    "R2 = skmet.r2_score(a,pred1.flatten())\n",
    "\n",
    "# Area under curve\n",
    "fig0  = plt.figure(figsize=(6,6))\n",
    "ax0 = fig0.add_subplot(111)\n",
    "ax0.plot(fpr,tpr)\n",
    "\n",
    "print('Confusion matrix')\n",
    "print(C1)\n",
    "print('Mean squared error, Area under curve')\n",
    "print(MSE1,AUC1)#,MSE2,MSE3,MSE4)\n",
    "print(\"-- %s seconds --\" % t)\n",
    "print('R2 score')\n",
    "print(R2)\n",
    "\n",
    "# Grade vs. predicted\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax2 = fig.add_subplot(111)\n",
    "ax2.scatter(a,pred1.flatten())\n",
    "ax2.plot(a,m*a,'-',color='r')\n",
    "ax2.set_xlabel('Actual grade')\n",
    "ax2.set_ylabel('Predicted')\n",
    "for k in range(len(grades[:,0])):\n",
    "    txt = filelist[k]\n",
    "    txt = txt[0:-4]\n",
    "    txt = txt+str(grades[k,0])\n",
    "    ax2.annotate(txt,xy=(a[k],pred1[k]),color='r')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
